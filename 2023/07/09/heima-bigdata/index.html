<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>heima_bigdata | Meteoric Blog</title><meta name="author" content="LiJunQi"><meta name="copyright" content="LiJunQi"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="ffffff"><meta name="description" content="0、前置章节1、环境介绍2、VMware准备Linux虚拟机3、VMware虚拟机系统设置3.1、主机名、IP、SSH免密登录开启node1，修改主机名为node1,并修改固定ip为: 192.168.88.131 12345678910111 #修改主机名2 hostnamectl set-hostname node134 #修改IP地址5 vim &#x2F;etc&#x2F;sysconfig&#x2F;network-">
<meta property="og:type" content="article">
<meta property="og:title" content="heima_bigdata">
<meta property="og:url" content="https://pia-can-fly.github.io/Test/2023/07/09/heima-bigdata/index.html">
<meta property="og:site_name" content="Meteoric Blog">
<meta property="og:description" content="0、前置章节1、环境介绍2、VMware准备Linux虚拟机3、VMware虚拟机系统设置3.1、主机名、IP、SSH免密登录开启node1，修改主机名为node1,并修改固定ip为: 192.168.88.131 12345678910111 #修改主机名2 hostnamectl set-hostname node134 #修改IP地址5 vim &#x2F;etc&#x2F;sysconfig&#x2F;network-">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg">
<meta property="article:published_time" content="2023-07-09T03:31:33.000Z">
<meta property="article:modified_time" content="2023-07-14T06:40:03.589Z">
<meta property="article:author" content="LiJunQi">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><link rel="shortcut icon" href="/Test/img/favicon.png"><link rel="canonical" href="https://pia-can-fly.github.io/Test/2023/07/09/heima-bigdata/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/Test/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/Test/',
  algolia: undefined,
  localSearch: {"path":"/Test/search.xml","preload":true,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"找不到您查询的内容：${query}","hits_stats":"共找到 ${hits} 篇文章"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":365,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":230},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#1f1f1f","position":"top-right"},
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'heima_bigdata',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-07-14 14:40:03'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', 'ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          const isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
          const isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
          const isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
          const hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

          if (t === undefined) {
            if (isLightMode) activateLightMode()
            else if (isDarkMode) activateDarkMode()
            else if (isNotSpecified || hasNoSupport) {
              const now = new Date()
              const hour = now.getHours()
              const isNight = hour <= 8 || hour >= 22
              isNight ? activateDarkMode() : activateLightMode()
            }
            window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
              if (saveToLocal.get('theme') === undefined) {
                e.matches ? activateDarkMode() : activateLightMode()
              }
            })
          } else if (t === 'light') activateLightMode()
          else activateDarkMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-double-row-display@1.00/cardlistpost.min.css"/>
<style>#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags:before {content:"\A";
  white-space: pre;}#recent-posts > .recent-post-item >.recent-post-info > .article-meta-wrap > .tags > .article-meta__separator{display:none}</style>
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.2"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/Test/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/Test/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/Test/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/Test/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/Test/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/Test/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/Test/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Test/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Test/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/Test/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/Test/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/Test/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg')"><nav id="nav"><span id="blog-info"><a href="/Test/" title="Meteoric Blog"><span class="site-name">Meteoric Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/Test/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/Test/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/Test/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/Test/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/Test/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/Test/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/Test/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/Test/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/Test/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">heima_bigdata</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-07-09T03:31:33.000Z" title="发表于 2023-07-09 11:31:33">2023-07-09</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-07-14T06:40:03.589Z" title="更新于 2023-07-14 14:40:03">2023-07-14</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">9.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>40分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="heima_bigdata"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="0、前置章节"><a href="#0、前置章节" class="headerlink" title="0、前置章节"></a>0、前置章节</h1><h2 id="1、环境介绍"><a href="#1、环境介绍" class="headerlink" title="1、环境介绍"></a>1、环境介绍</h2><h2 id="2、VMware准备Linux虚拟机"><a href="#2、VMware准备Linux虚拟机" class="headerlink" title="2、VMware准备Linux虚拟机"></a>2、VMware准备Linux虚拟机</h2><h2 id="3、VMware虚拟机系统设置"><a href="#3、VMware虚拟机系统设置" class="headerlink" title="3、VMware虚拟机系统设置"></a>3、VMware虚拟机系统设置</h2><h3 id="3-1、主机名、IP、SSH免密登录"><a href="#3-1、主机名、IP、SSH免密登录" class="headerlink" title="3.1、主机名、IP、SSH免密登录"></a>3.1、主机名、IP、SSH免密登录</h3><p>开启node1，修改主机名为node1,并修改固定ip为: 192.168.88.131</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1 #修改主机名</span><br><span class="line">2 hostnamectl set-hostname node1</span><br><span class="line">3</span><br><span class="line">4 #修改IP地址</span><br><span class="line">5 vim /etc/sysconfig/network-scripts/ifcfg-ens33</span><br><span class="line">6 IPADDR=&quot;192.168.88.101&quot;</span><br><span class="line">8 #重启网卡</span><br><span class="line">9 systemctl stop network</span><br><span class="line">10 systemctl start network </span><br><span class="line">11 #或者直接</span><br><span class="line">12 systemctl restart network</span><br></pre></td></tr></table></figure>

<p>修改后的ifcfg-ens33文件：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">TYPE=&quot;Ethernet&quot;</span><br><span class="line">PROXY_METHOD=&quot;none&quot;</span><br><span class="line">BROWSER_ONLY=&quot;no&quot;</span><br><span class="line">BOOTPROTO=&quot;static&quot;</span><br><span class="line">DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV4_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6INIT=&quot;yes&quot;</span><br><span class="line">IPV6_AUTOCONF=&quot;yes&quot;</span><br><span class="line">IPV6_DEFROUTE=&quot;yes&quot;</span><br><span class="line">IPV6_FAILURE_FATAL=&quot;no&quot;</span><br><span class="line">IPV6_ADDR_GEN_MODE=&quot;stable-privacy&quot;</span><br><span class="line">NAME=&quot;ens33&quot;</span><br><span class="line">UUID=&quot;e722b17e-2136-4501-83a7-df525c9211a7&quot;</span><br><span class="line">DEVICE=&quot;ens33&quot;</span><br><span class="line">ONBOOT=&quot;yes&quot;</span><br><span class="line">IPADDR=&quot;192.168.88.101&quot;</span><br><span class="line">NETMASK=&quot;255.255.255.0&quot;</span><br><span class="line">GATEWAY=&quot;192.168.88.2&quot;</span><br><span class="line">DNS1=&quot;192.168.88.2&quot;</span><br></pre></td></tr></table></figure>

<p>同样的操作启动node2和node3,</p>
<p>修改node2主机名为node2，设置ip为192.168.88.102</p>
<p>修改node2主机名为node3，设置ip为192.168.88.103</p>
<h3 id="3-2-配置主机名映射"><a href="#3-2-配置主机名映射" class="headerlink" title="3.2 配置主机名映射"></a>3.2 配置主机名映射</h3><p>1.在Windows系统中修改hosts文件(在mac系统中修改&#x2F;etc&#x2F;hosts文件)，填入如下内容:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.88.101 node1</span><br><span class="line">192.168.88.102 node2</span><br><span class="line">192.168.88.103 node3</span><br></pre></td></tr></table></figure>

<p>2.在3台Linux的&#x2F;etc&#x2F;hosts文件中，填入如下内容(3台都要添加)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">192.168.88.101 node1</span><br><span class="line">192.168.88.102 node2</span><br><span class="line">192.168.88.103 node3</span><br></pre></td></tr></table></figure>

<h3 id="3-3-配置SSH免密登录"><a href="#3-3-配置SSH免密登录" class="headerlink" title="3.3 配置SSH免密登录"></a>3.3 配置SSH免密登录</h3><p>后续安装的集群化软件，多数需要远程登录以及远程执行命令，我们可以简单起见，配置三台Linux服务器之间的免密码互相SSH登陆</p>
<p>1.在每一台机器都执行: <code>ssh-keygen -t rsa -b 4096</code>，一路回车到底即可</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">可查看秘钥所在位置</span></span><br><span class="line">[root@node1 ~]# cd .ssh</span><br><span class="line">[root@node1 .ssh]# ll</span><br><span class="line">总用量 8</span><br><span class="line">-rw-------. 1 root root 3247 7月   9 12:25 id_rsa</span><br><span class="line">-rw-r--r--. 1 root root  736 7月   9 12:25 id_rsa.pub</span><br></pre></td></tr></table></figure>

<p>2.在每一台机器都执行:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id node1</span><br><span class="line">ssh-copy-id node2</span><br><span class="line">ssh-copy-id node3</span><br></pre></td></tr></table></figure>

<p>3.执行完毕后，node1、 node2、 node3之间将完成root用户之间的免密互通</p>
<h3 id="3-4-创建hadoop用户并配置免密登录"><a href="#3-4-创建hadoop用户并配置免密登录" class="headerlink" title="3.4 创建hadoop用户并配置免密登录"></a>3.4 创建hadoop用户并配置免密登录</h3><p>后续大数据的软件，将不会以root用户启动(确保安全，养成良好的习惯)</p>
<p>我们为大数据的软件创建一个单独的用户hadoop，并为三台服务器同样配置hadoop用户的免密互通</p>
<p>1.在每一台机器执行: useradd hadoop,创建hadoop用户</p>
<p>2.在每一台机器执行: passwd hadoop,设置hadoop用户密码为123456</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 ~]# useradd hadoop</span><br><span class="line">[root@node1 ~]# passwd hadoop</span><br><span class="line">更改用户 hadoop 的密码 。</span><br><span class="line">新的 密码：</span><br><span class="line">无效的密码： 密码少于 8 个字符</span><br><span class="line">重新输入新的 密码：</span><br><span class="line">passwd：所有的身份验证令牌已经成功更新。</span><br></pre></td></tr></table></figure>



<p>3.在每一台机器均切换到hadoop用户: <code>su - hadoop</code>，并执行<code>ssh-keygen -t rsa -b 4096</code>,创建ssh密钥</p>
<p>4.在每一台机器均执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id node1</span><br><span class="line">ssh-copy-id node2</span><br><span class="line">ssh-copy-id node3</span><br></pre></td></tr></table></figure>

<h3 id="3-5-配置JDK环境"><a href="#3-5-配置JDK环境" class="headerlink" title="3.5 配置JDK环境"></a>3.5 配置JDK环境</h3><p>1.创建文件夹，用来部署JDK，将JDK和Tomcat都安装部署到: &#x2F;export&#x2F;server 内</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /export/server</span><br></pre></td></tr></table></figure>

<p>2.解压缩JDK安装文件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u351-linux-x64.tar.gz -C /export/server</span><br></pre></td></tr></table></figure>

<p>3.配置JDK的软链接</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /export/server/jdk1.8.0_361 /export/server/jdk</span><br></pre></td></tr></table></figure>

<p>4.配置JAVA_ HOME环境变量，以及将$JAVA_ HOME&#x2F;bin文件夹加入PATH环境变量中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">编辑/etc/profile文件</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line">export PATH=$PATH:$JAVA_HOME/bin</span><br></pre></td></tr></table></figure>

<p>5.生效环境变量;</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source /etc/profile</span><br></pre></td></tr></table></figure>

<p>6.配置java执行程序的软链接</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">删除系統自带的java程序</span></span><br><span class="line">rm -f /usr/bin/java</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">软链接我们自己安装的java程序</span></span><br><span class="line">ln -s /export/server/jdk/bin/java /usr/bin/java</span><br></pre></td></tr></table></figure>

<p>7.执行验证</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java -version</span><br><span class="line">javac -version</span><br></pre></td></tr></table></figure>

<p>8.直接在node1上复制粘贴jdk给node2和node3，然后重复步骤3~7</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r jdk1.8.0_361 node2:`pwd`/</span><br><span class="line">scp -r jdk1.8.0_361 node3:`pwd`/</span><br></pre></td></tr></table></figure>

<h3 id="3-6-防火墙、SELinux、时间同步"><a href="#3-6-防火墙、SELinux、时间同步" class="headerlink" title="3.6 防火墙、SELinux、时间同步"></a>3.6 防火墙、SELinux、时间同步</h3><p><strong>关闭防火墙和SELinux</strong></p>
<p>集群化软件之间需要通过端口互相通讯，为了避免出现网络不通的问题，我们可以简单的在集群内部关闭防火墙</p>
<blockquote>
<p>在每一台机器都执行 </p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br></pre></td></tr></table></figure>

<p>Linux有一个安全模块: SELinux，用以限制用户和程序的相关权限，来确保系统的安全稳定。<br>在当前，我们只需要关闭SELinux功能，避免导致后面的软件运行出现问题即可</p>
<blockquote>
<p>在每一台机器都执行</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/sysconfig/selinux</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">将第七行， SELINUX=enforcing 改为</span></span><br><span class="line">SELINUX=disabled</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">保存退出后，千万要注意disabled单词不要写错，不然无法启动系统</span> </span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重启虚拟机即可</span></span><br><span class="line">init 6</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">重启后课查看防火墙状态</span></span><br><span class="line">systemctl status firewalld</span><br></pre></td></tr></table></figure>

<p><strong>时间同步</strong></p>
<p>修改时区并配置自动时间同步</p>
<p>以下操作在三台Linux均执行</p>
<p>1.安装ntp软件</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y ntp</span><br></pre></td></tr></table></figure>

<p>2.更新时区</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -f /etc/localtime;sudo ln -s /usr/share/zoneinfo/Asia/Shanghai /etc/localtime</span><br></pre></td></tr></table></figure>

<p>3.同步时间</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ntpdate -u ntp.aliyun.com</span><br></pre></td></tr></table></figure>

<p>4.开启ntp服务并设置开机自启</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl start ntpd</span><br><span class="line">systemctl enable ntpd</span><br></pre></td></tr></table></figure>

<h3 id="3-7-快照配置"><a href="#3-7-快照配置" class="headerlink" title="3.7 快照配置"></a>3.7 快照配置</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/settingp.png" alt="配置快照"></p>
<h1 id="第一章、Hello大数据-amp-分布式"><a href="#第一章、Hello大数据-amp-分布式" class="headerlink" title="第一章、Hello大数据&amp;分布式"></a>第一章、Hello大数据&amp;分布式</h1><h1 id="第二章、分布式存储-Hadoop-HDFS"><a href="#第二章、分布式存储-Hadoop-HDFS" class="headerlink" title="第二章、分布式存储 Hadoop HDFS"></a>第二章、分布式存储 Hadoop HDFS</h1><h2 id="1、为什么需要分布式存储"><a href="#1、为什么需要分布式存储" class="headerlink" title="1、为什么需要分布式存储"></a>1、为什么需要分布式存储</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">①、数据量太大，单机存储能力有上限，需要靠数量来解决问题</span><br><span class="line">②、数量的提升带来的是网络传输、磁盘读写、CPU、内存等各方面的综合提升。 分布式组合在一起可以达到1+1&gt;2的效果</span><br></pre></td></tr></table></figure>

<h2 id="2、分布式的基础架构分析"><a href="#2、分布式的基础架构分析" class="headerlink" title="2、分布式的基础架构分析"></a>2、分布式的基础架构分析</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. 分布式系统常见的组织形式？</span><br><span class="line">去中心化模式：没有明确中心，大家协调工作</span><br><span class="line">中心化模式：有明确的中心，基于中心节点分配工作</span><br><span class="line"></span><br><span class="line">2. 什么是主从模式？</span><br><span class="line">主从模式（Master-Slaves）就是中心化模式，表示有一个主节点来作为管理者，管理协调下属一批从节点工作。</span><br><span class="line"></span><br><span class="line">3. Hadoop是哪种模式？</span><br><span class="line">主从模式（中心化模式）的架构</span><br></pre></td></tr></table></figure>

<h2 id="3、HDFS的基础架构"><a href="#3、HDFS的基础架构" class="headerlink" title="3、HDFS的基础架构"></a>3、HDFS的基础架构</h2><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/Hadoop_HDFS.png" alt="Hadoop_HDFS"></p>
<h3 id="3-1-HDFS是一个典型的主从模式架构"><a href="#3-1-HDFS是一个典型的主从模式架构" class="headerlink" title="3.1 HDFS是一个典型的主从模式架构"></a>3.1 <strong>HDFS是一个典型的主从模式架构</strong></h3><p>HDFS是Hadoop三大组件(HDFS、MapReduce、YARN)之一</p>
<ul>
<li>全称是：Hadoop Distributed File System（Hadoop分布式文件系统）</li>
<li>是Hadoop技术栈内提供的分布式数据存储解决方案</li>
<li>可以在多台服务器上构建存储集群，存储海量的数据</li>
</ul>
<h3 id="3-2-HDFS的基础架构"><a href="#3-2-HDFS的基础架构" class="headerlink" title="3.2 HDFS的基础架构"></a>3.2 <strong>HDFS的基础架构</strong></h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/HDFS_base1.png" alt="HDFS的基础架构1"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/HDFS_base2.png" alt="HDFS的基础架构2"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/HDFS_base3.png" alt="HDFS的基础架构3"></p>
<h3 id="3-3-总结"><a href="#3-3-总结" class="headerlink" title="3.3 总结"></a>3.3 <strong>总结</strong></h3><ol>
<li><p>什么是HDFS？</p>
<p>HDFS全称：Hadoop Distributed File System</p>
<p>是Hadoop三大组件（HDFS、MapReduce、YARN）之一</p>
<p>可在多台服务器上构建集群，提供分布式数据存储能力</p>
</li>
<li><p>HDFS中的架构角色有哪些？</p>
<p>NameNode：主角色，管理HDFS集群和DataNode角色</p>
<p>DataNode：从角色，负责数据的存储</p>
<p>SecondaryNameNode：辅助角色，协助NameNode整理元数据</p>
</li>
<li><p>HDFS的基础架构：</p>
</li>
</ol>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/HDFS_base4.png" alt="HDFS的基础架构4"></p>
<h2 id="4、HDFS集群环境部署"><a href="#4、HDFS集群环境部署" class="headerlink" title="4、HDFS集群环境部署"></a>4、HDFS集群环境部署</h2><h3 id="4-1-VMware虚拟机中部署"><a href="#4-1-VMware虚拟机中部署" class="headerlink" title="4.1 VMware虚拟机中部署"></a>4.1 VMware虚拟机中部署</h3><h4 id="4-1-1-安装包下载："><a href="#4-1-1-安装包下载：" class="headerlink" title="4.1.1 安装包下载："></a>4.1.1 <strong>安装包下载：</strong></h4><p>官方网址：<a href="https://hadoop.apache.org，课程使用当前最新的发行版：3.3.4版。">https://hadoop.apache.org，课程使用当前最新的发行版：3.3.4版。</a></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/tardowning1.png" alt="安装包下载1"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/tardowning2.png" alt="安装包下载2"></p>
<h4 id="4-1-2-集群规划"><a href="#4-1-2-集群规划" class="headerlink" title="4.1.2 集群规划"></a>4.1.2 <strong>集群规划</strong></h4><p>在前置准备章节，我们准备了基于VMware的三台虚拟机，其硬件配置如下。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/yingsetting1.png" alt="硬件配置1"></p>
<p>Hadoop HDFS的角色包含：</p>
<ul>
<li><p>NameNode，主节点管理者</p>
</li>
<li><p>DataNode，从节点工作者</p>
</li>
<li><p>SecondaryNameNode，主节点辅助</p>
</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/fuwuguihua1.png" alt="服务规划1"></p>
<h4 id="4-1-3-上传-amp-解压"><a href="#4-1-3-上传-amp-解压" class="headerlink" title="4.1.3 上传 &amp; 解压"></a>4.1.3 <strong>上传 &amp; 解压</strong></h4><blockquote>
<p><strong>请确认已经完成前置准备中的服务器创建、固定IP、防火墙关闭、Hadoop用户创建、SSH免密、JDK部署等操作。</strong></p>
</blockquote>
<ol>
<li><p>上传Hadoop安装包到node1节点中</p>
</li>
<li><p>解压缩安装包到&#x2F;export&#x2F;server&#x2F;中</p>
</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-3.3.4.tar.gz -C /export/server</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>构建软链接</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /export/server</span><br><span class="line">ln -s /export/server/hadoop-3.3.4 hadoop</span><br></pre></td></tr></table></figure>

<ol start="4">
<li>进入hadoop安装包内</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd hadoop</span><br></pre></td></tr></table></figure>

<h4 id="4-1-4-Hadoop安装包目录结构"><a href="#4-1-4-Hadoop安装包目录结构" class="headerlink" title="4.1.4 Hadoop安装包目录结构"></a>4.1.4 Hadoop安装包目录结构</h4><p>cd 进入Hadoop安装包内，通过ls -l命令查看文件夹内部结构</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/Hadoop_inside.png" alt="Hadoop内部结构"></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">各个文件夹含义如下</span><br><span class="line">bin，存放Hadoop的各类程序（命令）</span><br><span class="line">etc，存放Hadoop的配置文件</span><br><span class="line">sbin，管理员程序（super bin）</span><br><span class="line"></span><br><span class="line">include，C语言的一些头文件</span><br><span class="line">lib，存放Linux系统的动态链接库（.so文件）</span><br><span class="line">libexec，存放配置Hadoop系统的脚本文件（.sh和.cmd）</span><br><span class="line">licenses-binary，存放许可证文件</span><br><span class="line">share，存放二进制源码（Java jar包）</span><br></pre></td></tr></table></figure>

<h4 id="4-1-5-修改配置文件，应用自定义设置"><a href="#4-1-5-修改配置文件，应用自定义设置" class="headerlink" title="4.1.5 修改配置文件，应用自定义设置"></a>4.1.5 修改配置文件，应用自定义设置</h4><p><strong>配置HDFS集群，我们主要涉及到如下文件的修改</strong>：</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">workers：		配置从节点（DataNode）有哪些</span><br><span class="line">hadoop-env.sh：		配置Hadoop的相关环境变量</span><br><span class="line">core-site.xml：		Hadoop核心配置文件</span><br><span class="line">hdfs-site.xml：		HDFS核心配置文件</span><br><span class="line">这些文件均存在与<span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/etc/hadoop文件夹中。</span><br><span class="line"></span><br><span class="line">ps：<span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME是后续我们要设置的环境变量，其指代Hadoop安装文件夹即/export/server/hadoop</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>配置workers文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">进入配置文件目录</span></span><br><span class="line">cd etc/hadoop</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">编辑workers文件</span></span><br><span class="line">vim workers</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">填入如下内容</span></span><br><span class="line">node1</span><br><span class="line">node2</span><br><span class="line">node3</span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">填入的node1、node2、node3</span><br><span class="line">表明集群记录了三个从节点（DataNode）</span><br></pre></td></tr></table></figure>

<p><strong>配置hadoop-env.sh文件</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">填入如下内容</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line">export HADOOP_HOME=/export/server/hadoop</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export HADOOP_LOG_DIR=$HADOOP_HOME/logs</span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">JAVA<span class="built_in">_</span>HOME，指明JDK环境的位置在哪</span><br><span class="line">HADOOP<span class="built_in">_</span>HOME，指明Hadoop安装位置</span><br><span class="line">HADOOP<span class="built_in">_</span>CONF<span class="built_in">_</span>DIR，指明Hadoop配置文件目录位置</span><br><span class="line">HADOOP<span class="built_in">_</span>LOG<span class="built_in">_</span>DIR，指明Hadoop运行日志目录位置</span><br><span class="line">通过记录这些环境变量， 来指明上述运行时的重要信息</span><br></pre></td></tr></table></figure>

<p><strong>配置core-site.xml文件</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">在文件内部填入如下内容</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node1:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>131072<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">key：fs.defaultFS</span><br><span class="line">含义：HDFS文件系统的网络通讯路径</span><br><span class="line">值：hdfs://node1:8020</span><br><span class="line">协议为hdfs://</span><br><span class="line">namenode为node1</span><br><span class="line">namenode通讯端口为8020</span><br><span class="line"></span><br><span class="line">key：io.file.buffer.size</span><br><span class="line">含义：io操作文件缓冲区大小</span><br><span class="line">值：131072 bit</span><br></pre></td></tr></table></figure>

<ul>
<li>hdfs:&#x2F;&#x2F;node1:8020为整个HDFS内部的通讯地址，应用协议为hdfs:&#x2F;&#x2F;（Hadoop内置协议）</li>
<li>表明DataNode将和node1的8020端口通讯，node1是NameNode所在机器</li>
<li>此配置固定了node1必须启动NameNode进程</li>
</ul>
<p><strong>配置hdfs-site.xml文件</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"># 在文件内部填入如下内容</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir.perm<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>700<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/nn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1,node2,node3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>268435456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/dn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>针对hdfs-site.xml，简单分析一下配置文件的内容</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># 在文件内部填入如下内容</span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir.perm<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>700<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/nn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1,node2,node3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">key：dfs.datanode.data.dir.perm</span><br><span class="line">含义：hdfs文件系统，默认创建的文件权限设置</span><br><span class="line">值：700，即：rwx------</span><br><span class="line"></span><br><span class="line">key：dfs.namenode.name.dir</span><br><span class="line">含义：NameNode元数据的存储位置</span><br><span class="line">值：/data/nn，在node1节点的/data/nn目录下</span><br><span class="line"></span><br><span class="line">key：dfs.namenode.hosts</span><br><span class="line">含义：NameNode允许哪几个节点的DataNode连接（即允许加入集群）</span><br><span class="line">值：node1、node2、node3，这三台服务器被授权</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>针对hdfs-site.xml，简单分析一下配置文件的内容</strong></p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># 续接上部</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>268435456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.handler.count<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/dn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">key：dfs.blocksize</span><br><span class="line">含义：hdfs默认块大小</span><br><span class="line">值：268435456（256MB）</span><br><span class="line"></span><br><span class="line">key：dfs.namenode.handler.count</span><br><span class="line">含义：namenode处理的并发线程数</span><br><span class="line">值：100，以100个并行度处理文件系统的管理任务</span><br><span class="line"></span><br><span class="line">key：dfs.datanode.data.dir</span><br><span class="line">含义：从节点DataNode的数据存储目录</span><br><span class="line">值：/data/dn，即数据存放在node1、node2、node3，三台机器的/data/dn内</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="4-1-6-准备数据目录"><a href="#4-1-6-准备数据目录" class="headerlink" title="4.1.6 准备数据目录"></a>4.1.6 <strong>准备数据目录</strong></h4><p>根据下述2个配置项：</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/peizhixiang.png" alt="配置项"></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">namenode数据存放node1的/data/nn</span><br><span class="line">datanode数据存放node1、node2、node3的/data/dn</span><br></pre></td></tr></table></figure>

<p>所以应该</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node1节点：</span></span><br><span class="line">mkdir -p /data/nn</span><br><span class="line">mkdir -p /data/dn</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node2和node3节点：</span></span><br><span class="line">mkdir -p /data/dn</span><br></pre></td></tr></table></figure>

<h4 id="4-1-7-分发Hadoop文件夹"><a href="#4-1-7-分发Hadoop文件夹" class="headerlink" title="4.1.7 分发Hadoop文件夹"></a>4.1.7 <strong>分发Hadoop文件夹</strong></h4><p>目前，已经基本完成Hadoop的配置操作，可以从node1将hadoop安装文件夹远程复制到node2、node3</p>
<ul>
<li>分发</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node1执行如下命令</span></span><br><span class="line">cd /export/server</span><br><span class="line">scp -r hadoop-3.3.4 node2:`pwd`/</span><br><span class="line">scp -r hadoop-3.3.4 node3:`pwd`/</span><br></pre></td></tr></table></figure>

<ul>
<li>在node2执行，为hadoop配置软链接</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node2执行如下命令</span></span><br><span class="line">ln -s /export/server/hadoop-3.3.4 /export/server/hadoop</span><br></pre></td></tr></table></figure>

<ul>
<li>在node3执行，为hadoop配置软链接</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在node3执行如下命令</span></span><br><span class="line">ln -s /export/server/hadoop-3.3.4 /export/server/hadoop</span><br></pre></td></tr></table></figure>

<h4 id="4-1-8-配置环境变量"><a href="#4-1-8-配置环境变量" class="headerlink" title="4.1.8 配置环境变量"></a>4.1.8 配置环境变量</h4><p>为了方便我们操作Hadoop，可以将Hadoop的一些脚本、程序配置到PATH中，方便后续使用。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/serach_evn_addr.png" alt="查看配置环境位置图"></p>
<p>在Hadoop文件夹中的bin、sbin两个文件夹内有许多的脚本和程序，现在来配置一下环境变量</p>
<ol>
<li>vim &#x2F;etc&#x2F;profile</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在/etc/profile文件底部追加如下内容</span></span><br><span class="line">export HADOOP_HOME=/export/server/hadoop</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>在node2和node3配置同样的环境变量</li>
</ol>
<h4 id="4-1-9-授权为hadoop用户"><a href="#4-1-9-授权为hadoop用户" class="headerlink" title="4.1.9 授权为hadoop用户"></a>4.1.9 授权为hadoop用户</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop部署的准备工作基本完成</span><br><span class="line">为了确保安全，hadoop系统不以root用户启动，我们以普通用户hadoop来启动整个Hadoop服务</span><br><span class="line">所以，现在需要对文件权限进行授权。</span><br><span class="line">ps：请确保已经提前创建好了hadoop用户（前置准备章节中有讲述），并配置好了hadoop用户之间的免密登录</span><br></pre></td></tr></table></figure>

<ul>
<li>以root身份，在node1、node2、node3三台服务器上均执行如下命令:</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">以root身份，在三台服务器上均执行</span></span><br><span class="line">chown -R hadoop:hadoop /data</span><br><span class="line">chown -R hadoop:hadoop /export</span><br></pre></td></tr></table></figure>

<h4 id="4-2-0-格式化整个文件系统"><a href="#4-2-0-格式化整个文件系统" class="headerlink" title="4.2.0 格式化整个文件系统"></a>4.2.0 格式化整个文件系统</h4><p>前期准备全部完成，现在对整个文件系统执行初始化</p>
<ul>
<li>格式化namenode</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">确保以hadoop用户执行</span></span><br><span class="line">su - hadoop</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">格式化namenode</span></span><br><span class="line">hadoop namenode -format</span><br></pre></td></tr></table></figure>

<ul>
<li>启动</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">一键启动hdfs集群</span></span><br><span class="line">start-dfs.sh</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">一键关闭hdfs集群</span></span><br><span class="line">stop-dfs.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">如果遇到命令未找到的错误，表明环境变量未配置好，可以以绝对路径执行</span></span><br><span class="line">/export/server/hadoop/sbin/start-dfs.sh</span><br><span class="line">/export/server/hadoop/sbin/stop-dfs.sh</span><br></pre></td></tr></table></figure>

<h4 id="4-2-1-查看HDFS-WEBUI"><a href="#4-2-1-查看HDFS-WEBUI" class="headerlink" title="4.2.1 查看HDFS WEBUI"></a>4.2.1 查看HDFS WEBUI</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启动完成后，可以在浏览器打开：</span><br><span class="line">http://node1:9870，即可查看到hdfs文件系统的管理网页。</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/hdfs_file_web.png" alt="hdfs文件系统的管理网页"></p>
<h4 id="4-2-2-总结"><a href="#4-2-2-总结" class="headerlink" title="4.2.2 总结"></a>4.2.2 总结</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">部署Hadoop的关键点</span><br><span class="line">1. 上传、解压到/export/server，配置软链接</span><br><span class="line">2. 修改4份配置文件</span><br><span class="line">workers</span><br><span class="line">hadoop-env.sh</span><br><span class="line">core-site.xml</span><br><span class="line">hdfs-site.xml</span><br><span class="line">3. 分发到node2、node3，并设置环境变量</span><br><span class="line">4. 创建数据目录，并修改文件权限归属hadoop账户</span><br><span class="line">5. 启动，并查看WEB UI</span><br></pre></td></tr></table></figure>

<h4 id="4-2-3-思考"><a href="#4-2-3-思考" class="headerlink" title="4.2.3 思考"></a>4.2.3 思考</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">千辛万苦部署好了Hadoop的HDFS集群，如果中间出现误操作导致集群出现问题了怎么办？</span><br><span class="line">有没有什么方便的办法保存刚刚部署好的状态？</span><br><span class="line"></span><br><span class="line">虚拟机快照</span><br></pre></td></tr></table></figure>



<h2 id="5、HDFS的Shell操作"><a href="#5、HDFS的Shell操作" class="headerlink" title="5、HDFS的Shell操作"></a>5、HDFS的Shell操作</h2><h3 id="5-1-进程启停管理"><a href="#5-1-进程启停管理" class="headerlink" title="5.1 进程启停管理"></a>5.1 进程启停管理</h3><h4 id="5-1-1-一键启停脚本"><a href="#5-1-1-一键启停脚本" class="headerlink" title="5.1.1 一键启停脚本"></a>5.1.1 <strong>一键启停脚本</strong></h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Hadoop HDFS组件内置了HDFS集群的一键启停脚本。</span><br><span class="line"></span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/start-dfs.sh，一键启动HDFS集群</span><br><span class="line">执行原理：</span><br><span class="line">在执行此脚本的机器上，启动SecondaryNameNode</span><br><span class="line">读取core-site.xml内容（fs.defaultFS项），确认NameNode所在机器，启动NameNode</span><br><span class="line">读取workers内容，确认DataNode所在机器，启动全部DataNode</span><br><span class="line"></span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/stop-dfs.sh，一键关闭HDFS集群</span><br><span class="line">执行原理：</span><br><span class="line">在执行此脚本的机器上，关闭SecondaryNameNode</span><br><span class="line">读取core-site.xml内容（fs.defaultFS项），确认NameNode所在机器，关闭NameNode</span><br><span class="line">读取workers内容，确认DataNode所在机器，关闭全部NameNode</span><br></pre></td></tr></table></figure>

<h4 id="5-1-2-单进程启停"><a href="#5-1-2-单进程启停" class="headerlink" title="5.1.2 单进程启停"></a>5.1.2 <strong>单进程启停</strong></h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">除了一键启停外，也可以单独控制进程的启停。</span><br><span class="line">1. <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/hadoop-daemon.sh，此脚本可以单独控制所在机器的进程的启停</span><br><span class="line">用法：hadoop-daemon.sh (start|status|stop) (namenode|secondarynamenode|datanode)</span><br><span class="line"></span><br><span class="line">2. <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/hdfs，此程序也可以用以单独控制所在机器的进程的启停</span><br><span class="line">用法：hdfs --daemon (start|status|stop) (namenode|secondarynamenode|datanode)</span><br></pre></td></tr></table></figure>

<h4 id="5-1-3-总结"><a href="#5-1-3-总结" class="headerlink" title="5.1.3 总结"></a>5.1.3 <strong>总结</strong></h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 一键启停脚本可用</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/start-dfs.sh</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/stop-dfs.sh</span><br><span class="line">2. 独立进程启停可用</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/hadoop-daemon.sh</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/hdfs --daemon</span><br></pre></td></tr></table></figure>

<h3 id="5-2-文件系统操作命令"><a href="#5-2-文件系统操作命令" class="headerlink" title="5.2 文件系统操作命令"></a>5.2 文件系统操作命令</h3><h4 id="5-2-1-HDFS文件系统基本信息"><a href="#5-2-1-HDFS文件系统基本信息" class="headerlink" title="5.2.1 HDFS文件系统基本信息"></a>5.2.1 <strong>HDFS文件系统基本信息</strong></h4><p>HDFS作为分布式存储的文件系统，有其对数据的路径表达方式。</p>
<p>HDFS同Linux系统一样，均是以&#x2F;作为根目录的组织形式</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/mulujiegou.png" alt="目录结构图"></p>
<p>Linux：	&#x2F;usr&#x2F;local&#x2F;hello.txt</p>
<p>HDFS：		&#x2F;usr&#x2F;local&#x2F;hello.txt</p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">如何区分呢？</span><br><span class="line">Linux：file:///</span><br><span class="line">HDFS：hdfs://namenode:port/</span><br><span class="line">如上路径：</span><br><span class="line">Linux：file:///usr/local/hello.txt</span><br><span class="line">HDFS：hdfs://node1:8020/usr/local/hello.txt</span><br><span class="line"></span><br><span class="line">协议头file:/// 或 hdfs://node1:8020/可以省略</span><br><span class="line">需要提供Linux路径的参数，会自动识别为file://</span><br><span class="line">需要提供HDFS路径的参数，会自动识别为hdfs://</span><br><span class="line">除非你明确需要写或不写会有BUG，否则一般不用写协议头</span><br></pre></td></tr></table></figure>

<h4 id="5-2-2-HDFS文件系统介绍"><a href="#5-2-2-HDFS文件系统介绍" class="headerlink" title="5.2.2 HDFS文件系统介绍"></a>5.2.2 <strong>HDFS文件系统介绍</strong></h4><p>关于HDFS文件系统的操作命令，Hadoop提供了2套命令体系</p>
<ul>
<li>hadoop命令（老版本用法），用法：hadoop fs [generic options]</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/hadoop_cmd.png" alt="hadoop命令"></p>
<ul>
<li>hdfs命令（新版本用法），用法：hdfs dfs [generic options]</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/hdfs_cmd.png" alt="hdfs命令"></p>
<p>$\textcolor{red}{两者在文件系统操作上，用法完全一致用哪个都可以}$</p>
<p>$\textcolor{red}{某些特殊操作需要选择hadoop命令或hdfs命令讲到的时候具体分析}$</p>
<h4 id="5-2-3-创建文件夹"><a href="#5-2-3-创建文件夹" class="headerlink" title="5.2.3  创建文件夹"></a>5.2.3  创建文件夹</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir [-p] &lt;path&gt; ...</span><br><span class="line">hdfs dfs -mkdir [-p] &lt;path&gt; ...</span><br><span class="line">	path 为待创建的目录</span><br><span class="line">	-p选项的行为与Linux mkdir -p一致，它会沿着路径创建父目录。</span><br><span class="line"></span><br><span class="line">hadoop fs -mkdir -p /itcast/bigdata</span><br><span class="line">hdfs fs -mkdir -p /itheima/hadoop</span><br></pre></td></tr></table></figure>

<h4 id="5-2-4-查看指定目录下内容"><a href="#5-2-4-查看指定目录下内容" class="headerlink" title="5.2.4 查看指定目录下内容"></a>5.2.4 查看指定目录下内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -ls [-h] [-R] [&lt;path&gt; ...] </span><br><span class="line">hdfs dfs -ls [-h] [-R] [&lt;path&gt; ...] 	</span><br><span class="line">path 指定目录路径</span><br><span class="line">	-h 人性化显示文件size</span><br><span class="line">	-R 递归查看指定目录及其子目录</span><br></pre></td></tr></table></figure>

<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/chakanzidingneirong.png" alt="查看指定内容"></p>
<h4 id="5-2-5-上传文件到HDFS指定目录下"><a href="#5-2-5-上传文件到HDFS指定目录下" class="headerlink" title="5.2.5 上传文件到HDFS指定目录下"></a>5.2.5 上传文件到HDFS指定目录下</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;</span><br><span class="line">hdfs dfs -put [-f] [-p] &lt;localsrc&gt; ... &lt;dst&gt;</span><br><span class="line">-f 覆盖目标文件（已存在下）</span><br><span class="line">	-p 保留访问和修改时间，所有权和权限。</span><br><span class="line">	localsrc 本地文件系统（客户端所在机器）</span><br><span class="line">	dst 目标文件系统（HDFS）</span><br><span class="line"></span><br><span class="line">hadoop fs -put words.txt /itcast</span><br><span class="line">hdfs dfs -put file:///etc/profile hdfs://node1:8020/itcast</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="5-2-6-查看HDFS文件内容"><a href="#5-2-6-查看HDFS文件内容" class="headerlink" title="5.2.6 查看HDFS文件内容"></a>5.2.6 查看HDFS文件内容</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat &lt;src&gt; ... </span><br><span class="line">hdfs dfs -cat &lt;src&gt; ...</span><br><span class="line">        读取指定文件全部内容，显示在标准输出控制台。</span><br><span class="line">hadoop fs -cat /itcast/words.txt</span><br><span class="line">hdfs dfs -cat /itcast/profile</span><br><span class="line"></span><br><span class="line">读取大文件可以使用管道符配合more</span><br><span class="line">hadoop fs -cat &lt;src&gt; | more</span><br><span class="line">hdfs dfs -cat &lt;src&gt; | more</span><br></pre></td></tr></table></figure>

<h4 id="5-2-7-下载HDFS文件"><a href="#5-2-7-下载HDFS文件" class="headerlink" title="5.2.7 下载HDFS文件"></a>5.2.7 下载HDFS文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get [-f] [-p] &lt;src&gt; ... &lt;localdst&gt;</span><br><span class="line">hdfs dfs -get [-f] [-p] &lt;src&gt; ... &lt;localdst&gt;</span><br><span class="line">        下载文件到本地文件系统指定目录，localdst必须是目录</span><br><span class="line">        -f 覆盖目标文件（已存在下）</span><br><span class="line">        -p 保留访问和修改时间，所有权和权限。</span><br><span class="line"></span><br><span class="line">[root@node1 ~]# mkdir test</span><br><span class="line">[root@node1 ~]# cd test/</span><br><span class="line">[root@node1 test]# ll</span><br><span class="line">total 0</span><br><span class="line">[root@node1 test]# hadoop fs -get /itcast/zookeeper.out ./</span><br><span class="line">[root@node1 test]# ll</span><br><span class="line">total 20</span><br><span class="line">-rw-r--r-- 1 root root 18213 Aug 18 17:54 zookeeper.out</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="5-2-8-拷贝HDFS文件"><a href="#5-2-8-拷贝HDFS文件" class="headerlink" title="5.2.8 拷贝HDFS文件"></a>5.2.8 拷贝HDFS文件</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cp [-f] &lt;src&gt; ... &lt;dst&gt; </span><br><span class="line">hdfs dfs -cp [-f] &lt;src&gt; ... &lt;dst&gt;</span><br><span class="line">        -f 覆盖目标文件（已存在下）</span><br><span class="line"></span><br><span class="line">[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast</span><br><span class="line">[root@node3 ~]# hadoop fs -cp /small/1.txt /itcast/666.txt   #重命名</span><br><span class="line">[root@node3 ~]# hadoop fs -ls /itcast</span><br><span class="line">Found 4 items</span><br><span class="line">-rw-r--r--   3 root supergroup          2 2021-08-18 17:58 /itcast/1.txt</span><br><span class="line">-rw-r--r--   3 root supergroup          2 2021-08-18 17:59 /itcast/666.txt</span><br></pre></td></tr></table></figure>

<h4 id="5-2-9-追加数据到HDFS文件中"><a href="#5-2-9-追加数据到HDFS文件中" class="headerlink" title="5.2.9 追加数据到HDFS文件中"></a>5.2.9 追加数据到HDFS文件中</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;</span><br><span class="line">hdfs dfs -appendToFile &lt;localsrc&gt; ... &lt;dst&gt;</span><br><span class="line">        将所有给定本地文件的内容追加到给定dst文件。 </span><br><span class="line">        dst如果文件不存在，将创建该文件。 </span><br><span class="line">        如果&lt;localSrc&gt;为-，则输入为从标准输入中读取。</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">追加内容到文件尾部 appendToFile</span></span><br><span class="line">[root@node3 ~]# echo 1 &gt;&gt; 1.txt</span><br><span class="line">[root@node3 ~]# echo 2 &gt;&gt; 2.txt </span><br><span class="line">[root@node3 ~]# echo 3 &gt;&gt; 3.txt </span><br><span class="line">[root@node3 ~]# hadoop fs -put 1.txt /</span><br><span class="line">[root@node3 ~]# hadoop fs -cat /1.txt</span><br><span class="line">1</span><br><span class="line">[root@node3 ~]# hadoop fs -appendToFile 2.txt 3.txt /1.txt</span><br><span class="line">[root@node3 ~]# hadoop fs -cat /1.txt</span><br><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td></tr></table></figure>

<h4 id="5-2-10-HDFS数据移动操作"><a href="#5-2-10-HDFS数据移动操作" class="headerlink" title="5.2.10 HDFS数据移动操作"></a>5.2.10 HDFS数据移动操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mv &lt;src&gt; ... &lt;dst&gt;</span><br><span class="line">hdfs dfs -mv &lt;src&gt; ... &lt;dst&gt;	</span><br><span class="line">        移动文件到指定文件夹下</span><br><span class="line">        可以使用该命令移动数据，重命名文件的名称</span><br></pre></td></tr></table></figure>

<h4 id="5-2-11-HDFS数据删除操作"><a href="#5-2-11-HDFS数据删除操作" class="headerlink" title="5.2.11 HDFS数据删除操作"></a>5.2.11 HDFS数据删除操作</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -r [-skipTrash] URI [URI ...]</span><br><span class="line">hdfs dfs -rm -r [-skipTrash] URI [URI ...]	</span><br><span class="line">        删除指定路径的文件或文件夹</span><br><span class="line">        -skipTrash 跳过回收站，直接删除</span><br><span class="line"></span><br><span class="line">回收站功能默认关闭，如果要开启需要在core-site.xml内配置：</span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.trash.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;1440&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"> </span><br><span class="line">&lt;property&gt;</span><br><span class="line">&lt;name&gt;fs.trash.checkpoint.interval&lt;/name&gt;</span><br><span class="line">&lt;value&gt;120&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br><span class="line"></span><br><span class="line">无需重启集群，在哪个机器配置的，在哪个机器执行命令就生效。</span><br><span class="line">回收站默认位置在：/user/用户名(hadoop)/.Trash</span><br></pre></td></tr></table></figure>

<h4 id="5-2-12-HDFS-shell其它命令"><a href="#5-2-12-HDFS-shell其它命令" class="headerlink" title="5.2.12 HDFS shell其它命令"></a>5.2.12 HDFS shell其它命令</h4><ul>
<li><p>命令官方指导文档</p>
<p><a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/FileSystemShell.html">https://hadoop.apache.org/docs/r3.3.4/hadoop-project-dist/hadoop-common/FileSystemShell.html</a></p>
</li>
<li><p>提示</p>
<p>常见的操作自己最好能够记住，其他操作可以根据需要查询文档使用。</p>
<p>命令属于多用多会，孰能生巧，不用就忘。</p>
</li>
</ul>
<h4 id="5-2-13-HDFS-WEB浏览"><a href="#5-2-13-HDFS-WEB浏览" class="headerlink" title="5.2.13 HDFS WEB浏览"></a>5.2.13 HDFS WEB浏览</h4><p>除了使用命令操作HDFS文件系统外，在HDFS的WEB UI上也可以查看HDFS文件系统的内容。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/web1.png" alt="web1"></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/web2.png" alt="web2"></p>
<p>使用WEB浏览操作文件系统，一般会遇到权限问题</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/permission.png" alt="permission"></p>
<p>这是因为WEB浏览器中是以匿名用户（dr.who）登陆的，其只有只读权限，多数操作是做不了的。</p>
<p>如果需要以特权用户在浏览器中进行操作，需要配置如下内容到core-site.xml并重启集群</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.http.staticuser.user<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>$\textcolor{red}{但是，不推荐这样做}$</p>
<ul>
<li><p>$\textcolor{red}{HDFS WEBUI，只读权限挺好的，简单浏览即可}$</p>
</li>
<li><p>$\textcolor{red}{如果给与高权限，会有很大的安全问题，造成数据泄露或丢失}$</p>
</li>
</ul>
<h4 id="5-2-14-总结"><a href="#5-2-14-总结" class="headerlink" title="5.2.14 总结"></a>5.2.14 总结</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1. 命令操作可以使用</span><br><span class="line">hadoop fs</span><br><span class="line">hdfs dfs</span><br><span class="line"></span><br><span class="line">2. 文件系统协议包括</span><br><span class="line">file:// 表示Linux本地文件</span><br><span class="line">hdfs://namenode<span class="built_in">_</span>server:port/ 表示HDFS文件系统</span><br><span class="line">比如当前集群表示为：hdfs://node1:8020/</span><br><span class="line">一般可以省略file://和hdfs://协议头，不用写</span><br><span class="line"></span><br><span class="line">3. 常用命令</span><br><span class="line">mkdir 创建文件夹</span><br><span class="line">ls、cat 列出内容、查看内容</span><br><span class="line">cp、mv、rmr 复制、移动、删除</span><br><span class="line">put、get 上传、下载</span><br><span class="line">appendToFile 向文件追加内容</span><br><span class="line"></span><br><span class="line">4. 可以在HDFS WEBUI网页中进行HDFS文件系统的基本操作</span><br></pre></td></tr></table></figure>

<h3 id="5-3-HDFS权限"><a href="#5-3-HDFS权限" class="headerlink" title="5.3 HDFS权限"></a>5.3 HDFS权限</h3><p>HDFS Shell命令权限不足问题解决：</p>
<p><strong>Permission denied</strong>：</p>
<p>想必有同学在实战Shell的时候，遇到了：</p>
<p>Permission denied: user&#x3D;root, access&#x3D;WRITE, inode&#x3D;”&#x2F;“:hadoop:supergroup:drwxr-xr-x这种类似的问题。</p>
<p>问题的原因就是没有权限，那么为什么呢？</p>
<p><strong>HDFS超级用户</strong></p>
<p>如图所示</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/supergroup.png" alt="supergroup"></p>
<p>HDFS中，也是有权限控制的，其控制逻辑和Linux文件系统的完全一致。</p>
<p>(ps：如不理解上图红框内容的含义，建议查看补充知识点中关于Linux权限管控的内容)</p>
<p>但是不同的是，大家的Superuser不同（超级用户不同）</p>
<ul>
<li><p>Linux的超级用户是root</p>
</li>
<li><p>HDFS文件系统的超级用户：是启动namenode的用户（也就是课程的hadoop用户）</p>
<p>所以遇到此问题的同学，请确保你的HDFS操作命令是以：hadoop用户执行的，root用户在HDFS上其实没特权。</p>
</li>
</ul>
<p><strong>修改权限</strong></p>
<p>在HDFS中，可以使用和Linux一样的授权语句，即：chown和chmod</p>
<ul>
<li><p>修改所属用户和组：</p>
<p>hadoop fs -chown [-R] root:root &#x2F;xxx.txt</p>
<p>hdfs dfs -chown [-R] root:root &#x2F;xxx.txt</p>
</li>
<li><p>修改权限</p>
<p>hadoop fs -chmod [-R] 777 &#x2F;xxx.txt</p>
<p>hdfs dfs -chmod [-R] 777 &#x2F;xxx.txt</p>
</li>
</ul>
<p><strong>总结</strong></p>
<ol>
<li><p>HDFS文件系统使用和Linux一样逻辑的权限控制体系</p>
</li>
<li><p>HDFS的超级用户：</p>
<p>启动namenode的用户</p>
</li>
<li><p>可以使用</p>
<p>hadoop fs -chown 或 hdfs dfs -chown，修改所属</p>
<p>hadoop fs -chmod 或 hdfs dfs -chmod，修改权限</p>
</li>
</ol>
<h3 id="5-4-HDFS客户端-Jetbrians产品插件"><a href="#5-4-HDFS客户端-Jetbrians产品插件" class="headerlink" title="5.4 HDFS客户端 - Jetbrians产品插件"></a>5.4 HDFS客户端 - Jetbrians产品插件</h3><h3 id="5-5-HDFS客户端-NFS"><a href="#5-5-HDFS客户端-NFS" class="headerlink" title="5.5 HDFS客户端 - NFS"></a>5.5 HDFS客户端 - NFS</h3><h2 id="6、HDFS的存储原理"><a href="#6、HDFS的存储原理" class="headerlink" title="6、HDFS的存储原理"></a>6、HDFS的存储原理</h2><h1 id="第三章、MapReduce-amp-YARN入门"><a href="#第三章、MapReduce-amp-YARN入门" class="headerlink" title="第三章、MapReduce &amp; YARN入门"></a>第三章、MapReduce &amp; YARN入门</h1><h2 id="1、分布式计算概述"><a href="#1、分布式计算概述" class="headerlink" title="1、分布式计算概述"></a>1、分布式计算概述</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1. 什么是计算、分布式计算？</span><br><span class="line">计算：对数据进行处理，使用统计分析等手段得到需要的结果</span><br><span class="line">分布式计算：多台服务器协同工作，共同完成一个计算任务</span><br><span class="line">2. 分布式计算常见的2种工作模式</span><br><span class="line">分散-&gt;汇总  （MapReduce就是这种模式）</span><br><span class="line">中心调度-&gt;步骤执行 （大数据体系的Spark、Flink等是这种模式）</span><br></pre></td></tr></table></figure>

<h2 id="2、MapReduce概述"><a href="#2、MapReduce概述" class="headerlink" title="2、MapReduce概述"></a>2、MapReduce概述</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. 什么是MapReduce</span><br><span class="line">MapReduce是Hadoop中的分布式计算组件</span><br><span class="line">MapReduce可以以分散-&gt;汇总（聚合）模式执行分布式计算任务</span><br><span class="line">2. MapReduce的主要编程接口</span><br><span class="line">map接口，主要提供“分散”功能，由服务器分布式处理数据</span><br><span class="line">reduce接口，主要提供“汇总”功能，进行数据汇总统计得到结果</span><br><span class="line">MapReduce可供Java、Python等语言开发计算程序</span><br><span class="line">注：MapReduce尽管可以通过Java、Python等语言进行程序开发，但当下年代基本没人会写它的代码了，因为太过时了。   尽管MapReduce很老了，但现在仍旧活跃在一线，主要是Apache Hive框架非常火，而Hive底层就是使用的MapReduce。 所以对于MapReduce的代码开发，课程会简单扩展一下，但不会深入讲解，对MapReduce的底层原理会放在Hive之后，基于Hive做深入分析。</span><br><span class="line">3. MapReduce的运行机制</span><br><span class="line">将要执行的需求，分解为多个Map Task和Reduce Task</span><br><span class="line">将Map Task 和 Reduce Task分配到对应的服务器去执行</span><br></pre></td></tr></table></figure>

<h2 id="3、YARN概述"><a href="#3、YARN概述" class="headerlink" title="3、YARN概述"></a>3、YARN概述</h2><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. YARN是做什么的？</span><br><span class="line">YARN是Hadoop的一个组件</span><br><span class="line">用以做集群的资源（内存、CPU等）调度</span><br><span class="line">2. 为什么需要资源调度</span><br><span class="line">将资源统一管控进行分配可以提高资源利用率</span><br><span class="line">3. 程序如何在YARN内运行</span><br><span class="line">程序向YARN申请所需资源</span><br><span class="line">YARN为程序分配所需资源供程序使用</span><br><span class="line">4. MapReduce和YARN的关系</span><br><span class="line">YARN用来调度资源给MapReduce分配和管理运行资源</span><br><span class="line">所以，MapReduce需要YARN才能执行（普遍情况）</span><br></pre></td></tr></table></figure>

<h2 id="4、YARN架构"><a href="#4、YARN架构" class="headerlink" title="4、YARN架构"></a>4、YARN架构</h2><h3 id="4-1-核心架构"><a href="#4-1-核心架构" class="headerlink" title="4.1 核心架构"></a>4.1 核心架构</h3><p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/MSManager.png" alt="MSManager"></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1. YARN的架构有哪2个角色？</span><br><span class="line">主（Master）：ResourceManager</span><br><span class="line">从（Slave）：NodeManager</span><br><span class="line">2. 两个角色各自的功能是什么？</span><br><span class="line">ResourceManager： 管理、统筹并分配整个集群的资源</span><br><span class="line">NodeManager：管理、分配单个服务器的资源，即创建管理容器，由容器提供资源供程序使用</span><br><span class="line">3. 什么是YARN的容器？</span><br><span class="line">容器（Container）是YARN的NodeManager在所属服务器上分配资源的手段</span><br><span class="line">创建一个资源容器，即由NodeManager占用这部分资源</span><br><span class="line">然后应用程序运行在NodeManager创建的这个容器内</span><br><span class="line">应用程序无法突破容器的资源限制</span><br><span class="line">ps：容器是虚拟化的相关机制，后续我们会详细讲解</span><br></pre></td></tr></table></figure>

<h3 id="4-2-辅助架构"><a href="#4-2-辅助架构" class="headerlink" title="4.2 辅助架构"></a>4.2 辅助架构</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">YARN的架构有哪些角色</span><br><span class="line">核心角色：ResourceManager和NodeManager</span><br><span class="line">辅助角色：ProxyServer，保障WEB UI访问的安全性</span><br><span class="line">辅助角色：JobHistoryServer，记录历史程序运行信息和日志</span><br></pre></td></tr></table></figure>

<h2 id="5、MapReduce-amp-YARN-的部署"><a href="#5、MapReduce-amp-YARN-的部署" class="headerlink" title="5、MapReduce &amp; YARN 的部署"></a>5、MapReduce &amp; YARN 的部署</h2><h3 id="5-1-MapReduce配置文件"><a href="#5-1-MapReduce配置文件" class="headerlink" title="5.1 MapReduce配置文件"></a>5.1 MapReduce配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在 <span class="variable">$HADOOP_HOME</span>/etc/hadoop 文件夹内</span></span><br><span class="line">[hadoop@node1 ~]$ cd /export/server/hadoop/etc/hadoop</span><br><span class="line">[hadoop@node1 hadoop]$ vim mapred-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改：mapred-env.sh文件，添加如下环境变量</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line">export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000</span><br><span class="line">export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 修改：mapred-site.xml文件，添加如下配置信息 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>mapreduce的运行框架设置为YARN<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>历史服务器通讯端口为node1:10020<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>历史服务器web端口为node1的19888<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/mr-history/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>历史信息在HDFS的记录临时路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/mr-history/done<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>历史信息在HDFS的记录路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HAD0OP_MAPRED_HOME=$HADOOP_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>设置MapReduce的am的运行环境变量<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HAD0OP_MAPRED_HOME=$HADOOP_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>设置MapReduce的map任务的运行环境变量<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.env&lt;/ name&gt;</span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>HAD0OP_MAPRED_HOME=$HAD0OP_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>设置MapReduce的reduce任务的运行环境变量<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-2-YARN配置文件"><a href="#5-2-YARN配置文件" class="headerlink" title="5.2 YARN配置文件"></a>5.2 YARN配置文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">在 <span class="variable">$HADOOP_HOME</span>/etc/hadoop 文件夹内</span></span><br><span class="line">[hadoop@node1 ~]$ cd /export/server/hadoop/etc/hadoop</span><br><span class="line">[hadoop@node1 hadoop]$ vim yarn-env.sh</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改：yarn-env.sh文件，添加如下4行环境变量内容：</span></span><br><span class="line">export JAVA_HOME=/export/server/jdk</span><br><span class="line">export HADOOP_HOME=/export/server/hadoop</span><br><span class="line">export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop</span><br><span class="line">export HADOOP_LOG_DIR=$HADOOP_HOME/logs</span><br></pre></td></tr></table></figure>

<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- 修改：yarn-site.xml文件，添加如下配置信息 --&gt;</span></span><br><span class="line">[hadoop@node1 hadoop]$ vim yarn-site.xml</span><br><span class="line"></span><br><span class="line"><span class="meta">&lt;?xml version=<span class="string">&quot;1.0&quot;</span>?&gt;</span></span><br><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Site specific YARN configuration properties --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://node1:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>历史服务器URL<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.web-proxy.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1:8089<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>代理服务器主机和端口<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>开启日志聚合<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/tmp/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>程序日志HDFS的存储路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Resourcemanager设置在node1节点<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>选择公平调度器<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/nm-local<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Nodemanager中间数据本地存储路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/nm-log<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Nodemanager数据日志本地存储路径<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>在节点管理上保留日志文档的默认时间（以秒为单位）仅在禁用日志聚合时适用<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>为Mapreduce程序开启Shuffle服务<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>

<h3 id="5-3-分发配置文件"><a href="#5-3-分发配置文件" class="headerlink" title="5.3 分发配置文件"></a>5.3 分发配置文件</h3><p>MapReduce和YARN的配置文件修改好后，需要分发到其它的服务器节点中。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@node1 ~]$ cd /export/server/hadoop/etc/hadoop</span><br><span class="line">[hadoop@node1 hadoop]$ scp mapred-env.sh mapred-site.xml yarn-env.sh yarn-site.xml node2:`pwd`/</span><br><span class="line">[hadoop@node1 hadoop]$ scp mapred-env.sh mapred-site.xml yarn-env.sh yarn-site.xml node3:`pwd`/</span><br></pre></td></tr></table></figure>

<p>分发完成配置文件，就可以启动YARN的相关进程啦。</p>
<h3 id="5-4-集群启动命令介绍"><a href="#5-4-集群启动命令介绍" class="headerlink" title="5.4 集群启动命令介绍"></a>5.4 集群启动命令介绍</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">常用的进程启动命令如下：</span><br><span class="line">一键启动YARN集群： <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/start-yarn.sh</span><br><span class="line"> 会基于yarn-site.xml中配置的yarn.resourcemanager.hostname来决定在哪台机器上启动resourcemanager</span><br><span class="line"> 会基于workers文件配置的主机启动NodeManager</span><br><span class="line">一键停止YARN集群： <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/stop-yarn.sh</span><br><span class="line">在当前机器，单独启动或停止进程</span><br><span class="line"> <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/yarn --daemon start|stop resourcemanager|nodemanager|proxyserver</span><br><span class="line"> start和stop决定启动和停止</span><br><span class="line"> 可控制resourcemanager、nodemanager、proxyserver三种进程</span><br><span class="line">历史服务器启动和停止</span><br><span class="line"> <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/mapred --daemon start|stop historyserver</span><br></pre></td></tr></table></figure>

<h3 id="5-5-开始启动YARN集群"><a href="#5-5-开始启动YARN集群" class="headerlink" title="5.5 开始启动YARN集群"></a>5.5 开始启动YARN集群</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">在node1服务器，以hadoop用户执行</span><br><span class="line">首先执行：<span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/start-yarn.sh，一键启动所需的:</span><br><span class="line">	ResourceManager</span><br><span class="line">	NodeManager</span><br><span class="line">	ProxyServer（代理服务器）</span><br><span class="line">其次执行：<span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/mapred --daemon start historyserver 启动:</span><br><span class="line">	HistoryServer（历史服务器）</span><br></pre></td></tr></table></figure>

<h3 id="5-6-查看YARN的WEB-UI页面"><a href="#5-6-查看YARN的WEB-UI页面" class="headerlink" title="5.6 查看YARN的WEB UI页面"></a>5.6 查看YARN的WEB UI页面</h3><ul>
<li>打开 <a target="_blank" rel="noopener" href="http://node1:8088/">http://node1:8088</a> 即可看到YARN集群的监控页面（ResourceManager的WEB UI）</li>
</ul>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/Test/2023/07/09/heima-bigdata/YARN_WEBUI.png" alt="YARN_WEBUI"></p>
<p>$\textcolor{red}{在最后，可以给虚拟机打上快照，保存安装状态}$</p>
<h3 id="5-7-总结"><a href="#5-7-总结" class="headerlink" title="5.7 总结"></a>5.7 总结</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1. 部署集群需要做的操作</span><br><span class="line">修改YARN和MapReduce相关配置文件</span><br><span class="line">仅启动YARN的相关进程（MapReduce无需启动任何进程，需要时会运行在YARN内部（容器中））</span><br><span class="line">2. 查看YARN运行网页</span><br><span class="line">http://node1:8088</span><br></pre></td></tr></table></figure>

<h2 id="6、MapReduce-amp-YARN-初体验"><a href="#6、MapReduce-amp-YARN-初体验" class="headerlink" title="6、MapReduce &amp; YARN 初体验"></a>6、MapReduce &amp; YARN 初体验</h2><h3 id="6-1-一键启动脚本"><a href="#6-1-一键启动脚本" class="headerlink" title="6.1 一键启动脚本"></a>6.1 一键启动脚本</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">启动：</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/start-yarn.sh</span><br><span class="line">	从yarn-site.xml中读取配置，确定ResourceManager所在机器，并启动它</span><br><span class="line">	读取workers文件，确定机器，启动全部的NodeManager</span><br><span class="line">	在当前机器启动ProxyServer（代理服务器）</span><br><span class="line">关闭：</span><br><span class="line">	<span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/stop-yarn.sh</span><br></pre></td></tr></table></figure>

<h3 id="6-2-单进程启停"><a href="#6-2-单进程启停" class="headerlink" title="6.2 单进程启停"></a>6.2 单进程启停</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">除了一键启停外，也可以单独控制进程的启停。</span><br><span class="line"></span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/yarn，此程序也可以用以单独控制所在机器的进程的启停</span><br><span class="line">用法：yarn --daemon (start|stop) (resourcemanager|nodemanager|proxyserver)</span><br><span class="line"></span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/mapred，此程序也可以用以单独控制所在机器的历史服务器的启停</span><br><span class="line">用法：mapred --daemon (start|stop) historyserver</span><br></pre></td></tr></table></figure>

<h3 id="6-3-小结"><a href="#6-3-小结" class="headerlink" title="6.3 小结"></a>6.3 小结</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">1. 一键启停脚本可用</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/start-yarn.sh</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/sbin/stop-yarn.sh</span><br><span class="line"></span><br><span class="line">2. 独立进程启停可用</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/yarn --daemon</span><br><span class="line">控制resourcemanager、nodemanager、proxyserver</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/bin/mapred --daemon</span><br><span class="line">控制historyserver</span><br></pre></td></tr></table></figure>



<h3 id="6-4-掌握提交自带MapReduce示例程序到YARN运行"><a href="#6-4-掌握提交自带MapReduce示例程序到YARN运行" class="headerlink" title="6.4 掌握提交自带MapReduce示例程序到YARN运行"></a>6.4 掌握提交自带MapReduce示例程序到YARN运行</h3><h4 id="6-4-1-提交MapReduce程序至YARN运行"><a href="#6-4-1-提交MapReduce程序至YARN运行" class="headerlink" title="6.4.1 提交MapReduce程序至YARN运行"></a>6.4.1 提交MapReduce程序至YARN运行</h4><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">在部署并成功启动YARN集群后，我们就可以在YARN上运行各类应用程序了。</span><br><span class="line">YARN作为资源调度管控框架，其本身提供资源供许多程序运行，常见的有：</span><br><span class="line">MapReduce程序</span><br><span class="line">Spark程序</span><br><span class="line">Flink程序</span><br><span class="line">Spark和Flink是大数据后续的学习内容</span><br><span class="line"></span><br><span class="line">Hadoop官方内置了一些预置的MapReduce程序代码，我们无需编程，只需要通过命令即可使用。</span><br><span class="line">常用的有2个MapReduce内置程序：</span><br><span class="line">wordcount：单词计数程序。 </span><br><span class="line">统计指定文件内各个单词出现的次数</span><br><span class="line">pi：求圆周率</span><br><span class="line">通过蒙特卡罗算法（统计模拟法）求圆周率</span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">wordcount：单词计数程序。 </span><br><span class="line">统计指定文件内各个单词出现的次数</span><br><span class="line"></span><br><span class="line">这些内置的示例MapReduce程序代码，都在：</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar 这个文件内。</span><br><span class="line">可以通过 hadoop jar 命令来运行它，提交MapReduce程序到YARN中。</span><br><span class="line">语法： hadoop jar 程序文件 java类名 [程序参数] ... [程序参数]</span><br><span class="line"></span><br><span class="line">单词计数示例程序的功能很简单：</span><br><span class="line">给定数据输入的路径（HDFS）、给定结果输出的路径（HDFS）</span><br><span class="line">将输入路径内的数据中的单词进行计数，将结果写到输出路径</span><br><span class="line">我们可以准备一份数据文件，并上传到HDFS中。</span><br><span class="line"></span><br><span class="line">将内容保存到Linux中为words.txt文件，并上传到HDFS:</span><br><span class="line">hadoop fs -mkdir -p /input/wordcount</span><br><span class="line">hadoop fs -mkdir /output</span><br><span class="line">hadoop fs -put words.txt /input/wordcount/</span><br><span class="line"></span><br><span class="line">执行如下命令，提交示例MapReduce程序WordCount到YARN中执行:</span><br><span class="line">hadoop jar <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount hdfs://node1:8020/input/wordcount/ hdfs://node1:8020/output/wc1</span><br><span class="line"></span><br><span class="line">注意：</span><br><span class="line">参数wordcount，表示运行jar包中的单词计数程序（Java Class）</span><br><span class="line">参数1是数据输入路径（hdfs://node1:8020/input/wordcount/)</span><br><span class="line">参数2是结果输出路径(hdfs://node1:8020/output/wc1)， 需要确保输出的文件夹不存在</span><br><span class="line"></span><br><span class="line">提交程序后，可以在YARN的WEB UI页面看到运行中的程序（http://node1:8088/cluster/apps)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">执行完成后，可以查看HDFS上的输出结果:</span><br><span class="line">结果中有：<span class="built_in">_</span>SUCCESS文件，part-r-00000文件</span><br><span class="line"><span class="built_in">_</span>SUCCESS文件是标记文件，表示运行成功，本身是空文件</span><br><span class="line">part-r-00000，是结果文件，结果存储在以part开头的文件中</span><br><span class="line"></span><br><span class="line">执行完成后，可以借助历史服务器查看到程序的历史运行信息</span><br><span class="line">ps：如果没有启动历史服务器和代理服务器，此操作无法完成（页面信息由历史服务器提供，鼠标点击跳转到新网页功能由代理服务器提供）</span><br></pre></td></tr></table></figure>

<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pi：求圆周率</span><br><span class="line">通过蒙特卡罗算法（统计模拟法）求圆周率</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">可以执行如下命令，使用蒙特卡罗算法模拟计算求PI（圆周率）</span><br><span class="line">hadoop jar <span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 3 1000</span><br><span class="line">参数pi表示要运行的Java类，这里表示运行jar包中的求pi程序</span><br><span class="line">参数3，表示设置几个map任务</span><br><span class="line">参数1000，表示模拟求PI的样本数（越大求的PI越准确，但是速度越慢）</span><br><span class="line"></span><br><span class="line">运行完成，求得PI值（样本1000太小，不够精准，仅演示）</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="6-5-总结"><a href="#6-5-总结" class="headerlink" title="6.5 总结"></a>6.5 总结</h3><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1. Hadoop自带的MapReduce示例程序的代码jar包是</span><br><span class="line"><span class="built_in">$</span>HADOOP<span class="built_in">_</span>HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar</span><br><span class="line">2. 使用什么命令提交MapReduce程序到YARN中执行？</span><br><span class="line">hadoop jar 命令</span><br><span class="line">语法：hadoop jar 程序文件 java类名 [程序参数] ... [程序参数]</span><br><span class="line">3. 如何查看程序运行状态</span><br><span class="line">在YARN WEB页面中查看</span><br></pre></td></tr></table></figure>



<h1 id="第四章"><a href="#第四章" class="headerlink" title="第四章"></a>第四章</h1><h2 id="1、-Apache-Hive-概述"><a href="#1、-Apache-Hive-概述" class="headerlink" title="1、 Apache Hive 概述"></a>1、 Apache Hive 概述</h2><p>Apache Hive是一款分布式SQL计算的工具， 其主要功能是：</p>
<ul>
<li>将SQL语句 翻译成MapReduce程序运行</li>
</ul>
<p><strong>为什么使用Hive</strong></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">使用Hadoop MapReduce直接处理数据所面临的问题 </span><br><span class="line">人员学习成本太高 需要掌握java、Python等编程语言</span><br><span class="line">MapReduce实现复杂查询逻辑开发难度太大 </span><br><span class="line">使用Hive处理数据的好处 </span><br><span class="line">操作接口采用类SQL语法，提供快速开发的能力（简单、容易上手）</span><br><span class="line">底层执行MapReduce，可以完成分布式海量数据的SQL处理</span><br></pre></td></tr></table></figure>

<p><strong>总结</strong></p>
<figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1. 什么是分布式SQL计算？</span><br><span class="line">以分布式的形式，执行SQL语句，进行数据统计分析。</span><br><span class="line">2. Apache Hive是做什么的？</span><br><span class="line">很简单，将SQL语句翻译成MapReduce程序，从而提供用户分布式SQL计算的能力。</span><br><span class="line">传统MapReduce开发：写MR代码-&gt;得到结果</span><br><span class="line">使用Hive开发：写SQL-&gt;得到结果</span><br><span class="line">底层都是MR在运行，但是使用层面上更加简单了。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2、模拟实现Hive功能"><a href="#2、模拟实现Hive功能" class="headerlink" title="2、模拟实现Hive功能"></a>2、模拟实现Hive功能</h2><p>基于MapReduce构建分布式SQL执行引擎，主要需要有哪些功能组件？</p>
<ul>
<li>元数据管理</li>
<li>SQL解析器</li>
</ul>
<h2 id="3、Hive基础架构"><a href="#3、Hive基础架构" class="headerlink" title="3、Hive基础架构"></a>3、Hive基础架构</h2><h2 id="4、Hive部署"><a href="#4、Hive部署" class="headerlink" title="4、Hive部署"></a>4、Hive部署</h2><h2 id="5、Hive初体验"><a href="#5、Hive初体验" class="headerlink" title="5、Hive初体验"></a>5、Hive初体验</h2><h2 id="6、Hive客户端"><a href="#6、Hive客户端" class="headerlink" title="6、Hive客户端"></a>6、Hive客户端</h2><p>hive-site.xml</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node1:3306/hive?createDatabaseIfNotExist=true<span class="symbol">&amp;amp;</span>useSSL=false<span class="symbol">&amp;amp;</span>useUnicode=true<span class="symbol">&amp;amp;</span>characterEncoding=UTF-8<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.server2.thrift.bind.host<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>node1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.uris<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>thrift://node1:9083<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.event.db.notification.api.auth<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>





<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> test <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;王力宏&#x27;</span>,<span class="string">&#x27;男&#x27;</span>),(<span class="number">2</span>,<span class="string">&#x27;周杰伦&#x27;</span>,<span class="string">&#x27;男&#x27;</span>),(<span class="number">3</span>,<span class="string">&#x27;王菲&#x27;</span>,<span class="string">&#x27;女&#x27;</span>);</span><br></pre></td></tr></table></figure>



</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://pia-can-fly.github.io/Test">LiJunQi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://pia-can-fly.github.io/Test/2023/07/09/heima-bigdata/">https://pia-can-fly.github.io/Test/2023/07/09/heima-bigdata/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://pia-can-fly.github.io/Test" target="_blank">Meteoric Blog</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/Test/2023/07/08/%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0/" title="测试学习"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/Test/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">测试学习</div></div></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2021/02/24/5O1day2nriDzjSu.png" onerror="this.onerror=null;this.src='/Test/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">LiJunQi</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/Test/archives/"><div class="headline">文章</div><div class="length-num">9</div></a><a href="/Test/tags/"><div class="headline">标签</div><div class="length-num">3</div></a><a href="/Test/categories/"><div class="headline">分类</div><div class="length-num">0</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/pia-can-fly"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/pia-can-fly" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:ljq3497563545@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #4a7dbe;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">This is Meteoric Blog</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#0%E3%80%81%E5%89%8D%E7%BD%AE%E7%AB%A0%E8%8A%82"><span class="toc-text">0、前置章节</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E7%8E%AF%E5%A2%83%E4%BB%8B%E7%BB%8D"><span class="toc-text">1、环境介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81VMware%E5%87%86%E5%A4%87Linux%E8%99%9A%E6%8B%9F%E6%9C%BA"><span class="toc-text">2、VMware准备Linux虚拟机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81VMware%E8%99%9A%E6%8B%9F%E6%9C%BA%E7%B3%BB%E7%BB%9F%E8%AE%BE%E7%BD%AE"><span class="toc-text">3、VMware虚拟机系统设置</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1%E3%80%81%E4%B8%BB%E6%9C%BA%E5%90%8D%E3%80%81IP%E3%80%81SSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-text">3.1、主机名、IP、SSH免密登录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-%E9%85%8D%E7%BD%AE%E4%B8%BB%E6%9C%BA%E5%90%8D%E6%98%A0%E5%B0%84"><span class="toc-text">3.2 配置主机名映射</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E9%85%8D%E7%BD%AESSH%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-text">3.3 配置SSH免密登录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-%E5%88%9B%E5%BB%BAhadoop%E7%94%A8%E6%88%B7%E5%B9%B6%E9%85%8D%E7%BD%AE%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95"><span class="toc-text">3.4 创建hadoop用户并配置免密登录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-5-%E9%85%8D%E7%BD%AEJDK%E7%8E%AF%E5%A2%83"><span class="toc-text">3.5 配置JDK环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-6-%E9%98%B2%E7%81%AB%E5%A2%99%E3%80%81SELinux%E3%80%81%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5"><span class="toc-text">3.6 防火墙、SELinux、时间同步</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-7-%E5%BF%AB%E7%85%A7%E9%85%8D%E7%BD%AE"><span class="toc-text">3.7 快照配置</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0%E3%80%81Hello%E5%A4%A7%E6%95%B0%E6%8D%AE-amp-%E5%88%86%E5%B8%83%E5%BC%8F"><span class="toc-text">第一章、Hello大数据&amp;分布式</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8-Hadoop-HDFS"><span class="toc-text">第二章、分布式存储 Hadoop HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8"><span class="toc-text">1、为什么需要分布式存储</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84%E5%88%86%E6%9E%90"><span class="toc-text">2、分布式的基础架构分析</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81HDFS%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-text">3、HDFS的基础架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-HDFS%E6%98%AF%E4%B8%80%E4%B8%AA%E5%85%B8%E5%9E%8B%E7%9A%84%E4%B8%BB%E4%BB%8E%E6%A8%A1%E5%BC%8F%E6%9E%B6%E6%9E%84"><span class="toc-text">3.1 HDFS是一个典型的主从模式架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-HDFS%E7%9A%84%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-text">3.2 HDFS的基础架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E6%80%BB%E7%BB%93"><span class="toc-text">3.3 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81HDFS%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2"><span class="toc-text">4、HDFS集群环境部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-VMware%E8%99%9A%E6%8B%9F%E6%9C%BA%E4%B8%AD%E9%83%A8%E7%BD%B2"><span class="toc-text">4.1 VMware虚拟机中部署</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-1-%E5%AE%89%E8%A3%85%E5%8C%85%E4%B8%8B%E8%BD%BD%EF%BC%9A"><span class="toc-text">4.1.1 安装包下载：</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-2-%E9%9B%86%E7%BE%A4%E8%A7%84%E5%88%92"><span class="toc-text">4.1.2 集群规划</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-3-%E4%B8%8A%E4%BC%A0-amp-%E8%A7%A3%E5%8E%8B"><span class="toc-text">4.1.3 上传 &amp; 解压</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-4-Hadoop%E5%AE%89%E8%A3%85%E5%8C%85%E7%9B%AE%E5%BD%95%E7%BB%93%E6%9E%84"><span class="toc-text">4.1.4 Hadoop安装包目录结构</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-5-%E4%BF%AE%E6%94%B9%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6%EF%BC%8C%E5%BA%94%E7%94%A8%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%BE%E7%BD%AE"><span class="toc-text">4.1.5 修改配置文件，应用自定义设置</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-6-%E5%87%86%E5%A4%87%E6%95%B0%E6%8D%AE%E7%9B%AE%E5%BD%95"><span class="toc-text">4.1.6 准备数据目录</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-7-%E5%88%86%E5%8F%91Hadoop%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-text">4.1.7 分发Hadoop文件夹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-8-%E9%85%8D%E7%BD%AE%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F"><span class="toc-text">4.1.8 配置环境变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-1-9-%E6%8E%88%E6%9D%83%E4%B8%BAhadoop%E7%94%A8%E6%88%B7"><span class="toc-text">4.1.9 授权为hadoop用户</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-0-%E6%A0%BC%E5%BC%8F%E5%8C%96%E6%95%B4%E4%B8%AA%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F"><span class="toc-text">4.2.0 格式化整个文件系统</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-1-%E6%9F%A5%E7%9C%8BHDFS-WEBUI"><span class="toc-text">4.2.1 查看HDFS WEBUI</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-2-%E6%80%BB%E7%BB%93"><span class="toc-text">4.2.2 总结</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-2-3-%E6%80%9D%E8%80%83"><span class="toc-text">4.2.3 思考</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81HDFS%E7%9A%84Shell%E6%93%8D%E4%BD%9C"><span class="toc-text">5、HDFS的Shell操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%81%9C%E7%AE%A1%E7%90%86"><span class="toc-text">5.1 进程启停管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-1-%E4%B8%80%E9%94%AE%E5%90%AF%E5%81%9C%E8%84%9A%E6%9C%AC"><span class="toc-text">5.1.1 一键启停脚本</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-2-%E5%8D%95%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%81%9C"><span class="toc-text">5.1.2 单进程启停</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-1-3-%E6%80%BB%E7%BB%93"><span class="toc-text">5.1.3 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4"><span class="toc-text">5.2 文件系统操作命令</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-1-HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-text">5.2.1 HDFS文件系统基本信息</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-2-HDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E4%BB%8B%E7%BB%8D"><span class="toc-text">5.2.2 HDFS文件系统介绍</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-3-%E5%88%9B%E5%BB%BA%E6%96%87%E4%BB%B6%E5%A4%B9"><span class="toc-text">5.2.3  创建文件夹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-4-%E6%9F%A5%E7%9C%8B%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B%E5%86%85%E5%AE%B9"><span class="toc-text">5.2.4 查看指定目录下内容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-5-%E4%B8%8A%E4%BC%A0%E6%96%87%E4%BB%B6%E5%88%B0HDFS%E6%8C%87%E5%AE%9A%E7%9B%AE%E5%BD%95%E4%B8%8B"><span class="toc-text">5.2.5 上传文件到HDFS指定目录下</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-6-%E6%9F%A5%E7%9C%8BHDFS%E6%96%87%E4%BB%B6%E5%86%85%E5%AE%B9"><span class="toc-text">5.2.6 查看HDFS文件内容</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-7-%E4%B8%8B%E8%BD%BDHDFS%E6%96%87%E4%BB%B6"><span class="toc-text">5.2.7 下载HDFS文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-8-%E6%8B%B7%E8%B4%9DHDFS%E6%96%87%E4%BB%B6"><span class="toc-text">5.2.8 拷贝HDFS文件</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-9-%E8%BF%BD%E5%8A%A0%E6%95%B0%E6%8D%AE%E5%88%B0HDFS%E6%96%87%E4%BB%B6%E4%B8%AD"><span class="toc-text">5.2.9 追加数据到HDFS文件中</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-10-HDFS%E6%95%B0%E6%8D%AE%E7%A7%BB%E5%8A%A8%E6%93%8D%E4%BD%9C"><span class="toc-text">5.2.10 HDFS数据移动操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-11-HDFS%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4%E6%93%8D%E4%BD%9C"><span class="toc-text">5.2.11 HDFS数据删除操作</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-12-HDFS-shell%E5%85%B6%E5%AE%83%E5%91%BD%E4%BB%A4"><span class="toc-text">5.2.12 HDFS shell其它命令</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-13-HDFS-WEB%E6%B5%8F%E8%A7%88"><span class="toc-text">5.2.13 HDFS WEB浏览</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5-2-14-%E6%80%BB%E7%BB%93"><span class="toc-text">5.2.14 总结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-HDFS%E6%9D%83%E9%99%90"><span class="toc-text">5.3 HDFS权限</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF-Jetbrians%E4%BA%A7%E5%93%81%E6%8F%92%E4%BB%B6"><span class="toc-text">5.4 HDFS客户端 - Jetbrians产品插件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-HDFS%E5%AE%A2%E6%88%B7%E7%AB%AF-NFS"><span class="toc-text">5.5 HDFS客户端 - NFS</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81HDFS%E7%9A%84%E5%AD%98%E5%82%A8%E5%8E%9F%E7%90%86"><span class="toc-text">6、HDFS的存储原理</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%89%E7%AB%A0%E3%80%81MapReduce-amp-YARN%E5%85%A5%E9%97%A8"><span class="toc-text">第三章、MapReduce &amp; YARN入门</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%A1%E7%AE%97%E6%A6%82%E8%BF%B0"><span class="toc-text">1、分布式计算概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81MapReduce%E6%A6%82%E8%BF%B0"><span class="toc-text">2、MapReduce概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81YARN%E6%A6%82%E8%BF%B0"><span class="toc-text">3、YARN概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81YARN%E6%9E%B6%E6%9E%84"><span class="toc-text">4、YARN架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%A0%B8%E5%BF%83%E6%9E%B6%E6%9E%84"><span class="toc-text">4.1 核心架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-%E8%BE%85%E5%8A%A9%E6%9E%B6%E6%9E%84"><span class="toc-text">4.2 辅助架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81MapReduce-amp-YARN-%E7%9A%84%E9%83%A8%E7%BD%B2"><span class="toc-text">5、MapReduce &amp; YARN 的部署</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-MapReduce%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">5.1 MapReduce配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-YARN%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">5.2 YARN配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E5%88%86%E5%8F%91%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="toc-text">5.3 分发配置文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-4-%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E5%91%BD%E4%BB%A4%E4%BB%8B%E7%BB%8D"><span class="toc-text">5.4 集群启动命令介绍</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-5-%E5%BC%80%E5%A7%8B%E5%90%AF%E5%8A%A8YARN%E9%9B%86%E7%BE%A4"><span class="toc-text">5.5 开始启动YARN集群</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-6-%E6%9F%A5%E7%9C%8BYARN%E7%9A%84WEB-UI%E9%A1%B5%E9%9D%A2"><span class="toc-text">5.6 查看YARN的WEB UI页面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-7-%E6%80%BB%E7%BB%93"><span class="toc-text">5.7 总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81MapReduce-amp-YARN-%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-text">6、MapReduce &amp; YARN 初体验</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E4%B8%80%E9%94%AE%E5%90%AF%E5%8A%A8%E8%84%9A%E6%9C%AC"><span class="toc-text">6.1 一键启动脚本</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E5%8D%95%E8%BF%9B%E7%A8%8B%E5%90%AF%E5%81%9C"><span class="toc-text">6.2 单进程启停</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%B0%8F%E7%BB%93"><span class="toc-text">6.3 小结</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E6%8E%8C%E6%8F%A1%E6%8F%90%E4%BA%A4%E8%87%AA%E5%B8%A6MapReduce%E7%A4%BA%E4%BE%8B%E7%A8%8B%E5%BA%8F%E5%88%B0YARN%E8%BF%90%E8%A1%8C"><span class="toc-text">6.4 掌握提交自带MapReduce示例程序到YARN运行</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#6-4-1-%E6%8F%90%E4%BA%A4MapReduce%E7%A8%8B%E5%BA%8F%E8%87%B3YARN%E8%BF%90%E8%A1%8C"><span class="toc-text">6.4.1 提交MapReduce程序至YARN运行</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-%E6%80%BB%E7%BB%93"><span class="toc-text">6.5 总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0"><span class="toc-text">第四章</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1%E3%80%81-Apache-Hive-%E6%A6%82%E8%BF%B0"><span class="toc-text">1、 Apache Hive 概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2%E3%80%81%E6%A8%A1%E6%8B%9F%E5%AE%9E%E7%8E%B0Hive%E5%8A%9F%E8%83%BD"><span class="toc-text">2、模拟实现Hive功能</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3%E3%80%81Hive%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84"><span class="toc-text">3、Hive基础架构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4%E3%80%81Hive%E9%83%A8%E7%BD%B2"><span class="toc-text">4、Hive部署</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5%E3%80%81Hive%E5%88%9D%E4%BD%93%E9%AA%8C"><span class="toc-text">5、Hive初体验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6%E3%80%81Hive%E5%AE%A2%E6%88%B7%E7%AB%AF"><span class="toc-text">6、Hive客户端</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/Test/2023/07/09/heima-bigdata/" title="heima_bigdata"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/Test/img/404.jpg'" alt="heima_bigdata"/></a><div class="content"><a class="title" href="/Test/2023/07/09/heima-bigdata/" title="heima_bigdata">heima_bigdata</a><time datetime="2023-07-09T03:31:33.000Z" title="发表于 2023-07-09 11:31:33">2023-07-09</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Test/2023/07/08/%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0/" title="测试学习"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/Test/img/404.jpg'" alt="测试学习"/></a><div class="content"><a class="title" href="/Test/2023/07/08/%E6%B5%8B%E8%AF%95%E5%AD%A6%E4%B9%A0/" title="测试学习">测试学习</a><time datetime="2023-07-07T16:17:10.000Z" title="发表于 2023-07-08 00:17:10">2023-07-08</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Test/2023/07/07/%E5%AE%9E%E6%96%BD%E9%9D%A2%E8%AF%95/" title="实施面试"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/Test/img/404.jpg'" alt="实施面试"/></a><div class="content"><a class="title" href="/Test/2023/07/07/%E5%AE%9E%E6%96%BD%E9%9D%A2%E8%AF%95/" title="实施面试">实施面试</a><time datetime="2023-07-07T15:43:52.000Z" title="发表于 2023-07-07 23:43:52">2023-07-07</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Test/2023/07/06/sql%E7%BB%83%E4%B9%A0/" title="sql练习"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/Test/img/404.jpg'" alt="sql练习"/></a><div class="content"><a class="title" href="/Test/2023/07/06/sql%E7%BB%83%E4%B9%A0/" title="sql练习">sql练习</a><time datetime="2023-07-06T06:45:45.000Z" title="发表于 2023-07-06 14:45:45">2023-07-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/Test/2023/06/29/bigdata01/" title="bigdata01"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="this.onerror=null;this.src='/Test/img/404.jpg'" alt="bigdata01"/></a><div class="content"><a class="title" href="/Test/2023/06/29/bigdata01/" title="bigdata01">bigdata01</a><time datetime="2023-06-29T11:22:10.000Z" title="发表于 2023-06-29 19:22:10">2023-06-29</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 By LiJunQi</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/Test/js/utils.js"></script><script src="/Test/js/main.js"></script><script src="/Test/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script id="canvas_nest" defer="defer" color="0,0,255" opacity="0.7" zIndex="-1" count="30" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-nest.min.js"></script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="true" async="async"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/Test/js/search/local-search.js"></script></div></div><script src="/Test/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/Test/live2dw/assets/koharu.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false});</script></body></html>